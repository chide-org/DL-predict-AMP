{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422767fa",
   "metadata": {},
   "source": [
    "# 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc074c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wmt19 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'zh-en' at C:\\Users\\21198\\.cache\\huggingface\\datasets\\wmt19\\zh-en\\0.0.0\\c8a87a86736e7bf10c74c22e9033eba9146774e9 (last modified on Wed Jul  9 13:39:27 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 25984574\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3981\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 3981\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Training hyperparameters\n",
    "TOTAL_DATA_NUM = 2000\n",
    "\n",
    "MIN_FREQ = 1\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "learning_rate = 3e-4\n",
    "batch_first = True\n",
    "pad_idx = 0\n",
    "load_model = True\n",
    "save_model = True\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_size = 256\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 1024\n",
    "max_len = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device {device}')\n",
    "\n",
    "\n",
    "# load tokenize model\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_zh = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# tokenize\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en(text)]\n",
    "def tokenize_zh(text):\n",
    "    return [tok.text.lower() for tok in spacy_zh(text)]\n",
    "\n",
    "# dataset zh-en\n",
    "raw_datasets = load_dataset(\"wmt19\", \"zh-en\")\n",
    "print(raw_datasets) # 25984574 3981\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"].select(range(TOTAL_DATA_NUM))\n",
    "vali_dataset = raw_datasets[\"validation\"]\n",
    "print(train_dataset)\n",
    "print(vali_dataset)\n",
    "\n",
    "# tokenize dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        self.tokenizer_zh = get_tokenizer('spacy', language='zh_core_web_sm')\n",
    "        self.en_lines = []\n",
    "        self.zh_lines = []\n",
    "        for data in dataset:\n",
    "            self.en_lines.append(data['translation']['en'])\n",
    "            self.zh_lines.append(data['translation']['zh'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        en_line = self.en_lines[idx]\n",
    "        zh_line = self.zh_lines[idx]\n",
    "        en_tokens = self.tokenizer_en(en_line)\n",
    "        zh_tokens = self.tokenizer_zh(zh_line)\n",
    "        return en_tokens, zh_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "827d21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7324\n",
      "6916\n"
     ]
    }
   ],
   "source": [
    "# build vocab (opt)\n",
    "from torch.utils.data import ConcatDataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "\n",
    "en_vocab = {\n",
    "    '<pad>':0,\n",
    "    '<sos>':1,\n",
    "    '<eos>':2,\n",
    "    '<unk>':3\n",
    "}\n",
    "zh_vocab = {\n",
    "    '<pad>':0,\n",
    "    '<sos>':1,\n",
    "    '<eos>':2,\n",
    "    '<unk>':3\n",
    "}\n",
    "\n",
    "tk_dataset = WordDataset(train_dataset)\n",
    "en_all_tokens = list(itertools.chain.from_iterable(en_tokens for en_tokens, zh_tokens in tk_dataset))\n",
    "zh_all_tokens = list(itertools.chain.from_iterable(zh_tokens for en_tokens, zh_tokens in tk_dataset))\n",
    "en_vocab_counter = Counter(en_all_tokens)\n",
    "zh_vocab_counter = Counter(zh_all_tokens)\n",
    "# print(len(en_all_tokens))\n",
    "# print(len(zh_all_tokens))\n",
    "\n",
    "\n",
    "en_tot = 4\n",
    "for index, (token, freq) in enumerate(en_vocab_counter.items()):\n",
    "    if freq >= MIN_FREQ:\n",
    "        en_vocab.update({token: en_tot})\n",
    "        en_tot += 1  \n",
    "zh_tot = 4\n",
    "for index, (token, freq) in enumerate(zh_vocab_counter.items()):\n",
    "    if freq >= MIN_FREQ:\n",
    "        zh_vocab.update({token: zh_tot})\n",
    "        zh_tot += 1    \n",
    "print(len(en_vocab))\n",
    "print(len(zh_vocab))\n",
    "\n",
    "\n",
    "with open('vocab.en', \"wb\") as en_vocab_file:\n",
    "    pickle.dump(en_vocab, en_vocab_file)\n",
    "with open('vocab.zh', \"wb\") as zh_vocab_file: # write binary\n",
    "    pickle.dump(zh_vocab, zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "297e5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader for model forward train\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# word to number with vocab -- collate_fn\n",
    "def collate_fn(batch):\n",
    "    \"\"\"批处理函数，处理变长序列\"\"\"\n",
    "    zh_batch, en_batch = [], []\n",
    "    \n",
    "    for en_tokens, zh_tokens in batch:\n",
    "        \n",
    "        en_itokens = [en_vocab.get(token, en_vocab['<unk>']) for token in en_tokens] + [en_vocab.get('<eos>', 2)]\n",
    "        zh_itokens = [zh_vocab.get(token, zh_vocab['<unk>']) for token in zh_tokens] + [zh_vocab.get('<eos>', 2)]\n",
    "        \n",
    "        en_batch.append(torch.tensor(en_itokens, dtype=torch.long))\n",
    "        zh_batch.append(torch.tensor(zh_itokens, dtype=torch.long))\n",
    "    \n",
    "    en_batch = pad_sequence(en_batch, padding_value=en_vocab['<pad>'], batch_first=batch_first) # 用0填充短序列\n",
    "    zh_batch = pad_sequence(zh_batch, padding_value=zh_vocab['<pad>'], batch_first=batch_first)\n",
    "    \n",
    "    return  en_batch, zh_batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    WordDataset(train_dataset),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn = collate_fn,\n",
    "    \n",
    ")\n",
    "vali_dataloader = DataLoader(\n",
    "    WordDataset(vali_dataset),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn = collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae899d0",
   "metadata": {},
   "source": [
    "# 准备模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc74dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_size,\n",
    "            src_vocab_size,\n",
    "            trg_vocab_size,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            pad_idx,\n",
    "            dim_feedforward,\n",
    "            max_len,\n",
    "            device\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def make_padding_mask(self, N_S): # True will be ignored\n",
    "        padding_mask = N_S == self.pad_idx\n",
    "        return padding_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        batch_size, src_seq_length = src.shape\n",
    "        batch_size, trg_seq_length = trg.shape\n",
    "\n",
    "        src_position = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, batch_size)\n",
    "            .transpose(0,1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        trg_position = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, batch_size)\n",
    "            .transpose(0,1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        \n",
    "        src_embed = self.dropout(\n",
    "            self.src_word_embedding(src) + self.src_position_embedding(src_position)\n",
    "        ) \n",
    "        trg_embed = self.dropout(\n",
    "            self.trg_word_embedding(trg) + self.trg_position_embedding(trg_position)\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_padding_mask(src)\n",
    "        trg_padding_mask = self.make_padding_mask(trg)\n",
    "        trg_mask = (\n",
    "            torch.triu(\n",
    "                torch.ones(trg_seq_length, trg_seq_length, dtype=torch.bool, device=self.device),\n",
    "                diagonal=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_embed,\n",
    "            trg_embed,\n",
    "            src_key_padding_mask = src_padding_mask,\n",
    "            tgt_key_padding_mask = trg_padding_mask,\n",
    "            tgt_mask = trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5691b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 翻译测试函数\n",
    "def translate_sentence(model, sentence, en_vocab, zh_vocab, device, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # 分词并转换为索引\n",
    "    tokens = [en_vocab.get(token, en_vocab['<unk>']) for token in tokenize_en(sentence)] + [en_vocab.get('<eos>', 2)]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    # 初始化目标序列\n",
    "    outputs = [zh_vocab.get('<sos>', 1)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            trg_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "            best_guess = output.argmax(2)[:,-1].item()\n",
    "            outputs.append(best_guess)\n",
    "            \n",
    "            if best_guess == zh_vocab.get('<eos>', 2):\n",
    "                print(\"FIND EOS\")\n",
    "                break\n",
    "    \n",
    "    # 转换索引为词汇\n",
    "    zh_vocab_reverse = {v: k for k, v in zh_vocab.items()}\n",
    "    translated_sentence = [zh_vocab_reverse.get(idx, '<unk>') for idx in outputs[1:]]\n",
    "    return ''.join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af7a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换函数\n",
    "def translate_to_sentence(tokens, vocab, is_chinese=False):\n",
    "    vocab_reverse = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    if hasattr(tokens, 'cpu'):\n",
    "        token_list = tokens.cpu().numpy().tolist()\n",
    "    else:\n",
    "        token_list = tokens.tolist()\n",
    "    \n",
    "    # 转换为词汇\n",
    "    words = []\n",
    "    for token_id in token_list:\n",
    "        word = vocab_reverse.get(token_id, '<unk>')\n",
    "        words.append(word)\n",
    "        if word == '<eos>':  # 遇到结束符就停止\n",
    "            break\n",
    "    \n",
    "    # 根据语言类型连接\n",
    "    if is_chinese:\n",
    "        return ''.join(words)  # 中文不需要空格\n",
    "    else:\n",
    "        return ' '.join(words)  # 英文用空格连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df82cc",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ea270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa003c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21198\\AppData\\Local\\Temp\\ipykernel_28136\\1781684520.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "=> Saving checkpoint\n",
      "测试句子 (英文): 1929 or 1989?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21198\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\b\\abs_dd5ga5p9x7\\croot\\libtorch_1746637520098\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIND EOS\n",
      "翻译结果: 的的的的的的的的的的的的的的的的的的的的的的的。<eos>\n",
      "参考翻译: 1929年还是1989年?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████| 32/32 [00:53<00:00,  1.67s/it, Loss=6.6952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): It not only put financial markets and currencies at risk; it also exposed serious regulatory and governance shortcomings that have yet to be fully addressed.\n",
      "FIND EOS\n",
      "翻译结果: 在，，，，，的，，，的的的的的的的的。<eos>\n",
      "参考翻译: 它不仅对金融市场和货币构成威胁；而且还暴露了迄今为止都无法完全解决的严重的监管和治理缺陷。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████| 32/32 [00:57<00:00,  1.81s/it, Loss=6.7379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): What Failed in 2008?\n",
      "FIND EOS\n",
      "翻译结果: 但这一<eos>\n",
      "参考翻译: 2008年败在何处？\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████| 32/32 [01:02<00:00,  1.94s/it, Loss=6.1660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): STOCKHOLM/MADRID – When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother – pleasant and rich with experience, but lacking the vitality and energy of the past.\n",
      "翻译结果: 在，在在在在，在在在在，在在在在在在在，而的————————————————————————————\n",
      "参考翻译: 斯托克霍姆/马德里—去年11月，教皇方济各在其欧洲议会演讲中将欧盟比作祖母 — — 和蔼而富于经验，但已不再有过去的活力和能量。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████| 32/32 [01:01<00:00,  1.93s/it, Loss=6.1329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): President Barack Obama now might get so-called fast-track negotiating authority from Congress.\n",
      "FIND EOS\n",
      "翻译结果: 但这一是一一的是一一。<eos>\n",
      "参考翻译: 现在，美国总统奥巴马可能将从国会获得所谓的快速谈判授权。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████| 32/32 [01:01<00:00,  1.91s/it, Loss=6.1308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): In the coming years, such rays may become scarcer still.\n",
      "FIND EOS\n",
      "翻译结果: 但在在在在在在在在在在在在在在在在在，而。<eos>\n",
      "参考翻译: 而在未来几年，这样的曙光会变得越来越稀有。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████| 32/32 [01:02<00:00,  1.94s/it, Loss=6.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): As each country’s GDP growth slowed, so did its imports, causing its trading partners’ growth to slow as well.\n",
      "FIND EOS\n",
      "翻译结果: 但这一种，但这一种，但这一种，但这一种的国家的经济和其他。<eos>\n",
      "参考翻译: 随着各国GDP增长的放缓，进口也开始放缓，贸易伙伴的增长也随之放缓。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████████████████████████████████████| 32/32 [01:01<00:00,  1.93s/it, Loss=5.7993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): But, if it seemed clear that there would be political consequences, their form and timing were far less obvious.\n",
      "FIND EOS\n",
      "翻译结果: 但这一场，并在这一种，而不是是，并将会了我们。<eos>\n",
      "参考翻译: 但如果这似乎必然预示着某些政治后果，那么其形式和时间就不那么明显了。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████| 32/32 [01:01<00:00,  1.93s/it, Loss=5.5954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): During the heyday of Irish terrorism in the UK, successive British governments went out of their way not to concede to the IRA the notion that a war was being waged.\n",
      "FIND EOS\n",
      "翻译结果: 但这一种的，但在在欧洲的经济的情况下，而不是是一个，而不是的经济的经济的情况下，而不是。<eos>\n",
      "参考翻译: 当爱尔兰恐怖主义在英国猖獗之际，各届英国政府都竭力不屈从于爱尔兰共和军的观点，即正在进行一场战争。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████| 32/32 [01:02<00:00,  1.96s/it, Loss=5.5708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Second, and more importantly, our leaders must seek to calm, rather than exploit, public anxiety.\n",
      "FIND EOS\n",
      "翻译结果: 我们的是，我们必须在欧洲的，而不是的，而不是、、更多的情况下，而不是、更多的问题。<eos>\n",
      "参考翻译: 第二，也是更为重要的是，我们的领导人应当力求平息、而不是利用公众的焦虑。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=5.4979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): The question raised in October 2003 by then US Secretary of Defense Donald Rumsfeld is no less relevant today: “Are we capturing, killing, or deterring and dissuading more terrorists every day than the madrassas and the radical clerics are recruiting, training, and deploying against us?”\n",
      "FIND EOS\n",
      "翻译结果: 在许多国家，我们在过去和其他国家的“我们，我们在过去、“我们、“、以及在过去、设计，而不是、、“、、“政府和其他国家的“我们的国家。<eos>\n",
      "参考翻译: 2003年10月，时任美国国防部长拉姆斯菲尔德曾提出一个问题，至今仍很中肯 ： “ 我们每天俘虏、击毙、关押和改造过来的恐怖分子，能比伊斯兰学校和激进宗教人士每天招募、训练和派遣出来反对我们的人数多吗？\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.93s/it, Loss=5.1450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): While a Brazilian orange admitted for sale in Portugal can be sold throughout the EU, a Brazilian engineer or accountant licensed in Portugal must fulfill separate licensing requirements to work elsewhere in the EU, hampering much-needed labor mobility by forcing non-European workers to endure costly and inefficient bureaucratic procedures.\n",
      "翻译结果: 在我的是，在在在在过去的国家，因为我们在过去的国家，因为在在在过去的经济上，而不是是一个在过去的国家的新的国家，而不是，而不是，而不是\n",
      "参考翻译: 尽管获准在葡萄牙出售的巴西橙可以在整个欧盟出售，但获得葡萄牙执照的巴西工程师和会计师仍必须满足欧盟其他地区各自的执业要求，这迫使非欧洲工人忍受费时费力的官僚主义流程，不利于急需的劳动力流动。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.91s/it, Loss=5.1983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): If foreign financing is not forthcoming, yields on US government debt will rise and the US economy will fall back into recession.\n",
      "FIND EOS\n",
      "翻译结果: 如果日本的是，我们必须在美国和经济将会将会受到了我们的经济和其他。<eos>\n",
      "参考翻译: 如果得不到外国资金，美国政府债务的收益率将被迫提升而美国经济也将重新陷入衰退。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.88s/it, Loss=4.9395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): But, until now, it had to lend massive amounts to banks without being able to judge their soundness, because all of that information was in the hands of national authorities who guarded it jealously and typically denied problems until it was too late.\n",
      "翻译结果: 但是，如果没有，因为它在过去在过去的情况下，但这一次，但如果在过去个国家，但对欧洲议会的人，而不是对欧洲议会对其他国家的情况下，但这一种\n",
      "参考翻译: 但是，迄今为止，欧洲央行仍必须在无法判断银行稳健程度的情况下向它们大规模贷款，因为所有信息都掌握在国家级监管者手中，而后者对此死抓不放，出了问题总是拼命掩盖，直到无法收场为止。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=4.9390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): The good news is that the tax hike’s impact is temporary. Soon, it will begin to taper off, and industrial output will approach full capacity.\n",
      "FIND EOS\n",
      "翻译结果: 如果这一种政策，将会被视为，而不是在这一种，而不是是因为它的一种，而不是是因为它。<eos>\n",
      "参考翻译: 好消息是增税的影响只是暂时的，并将很快开始逐渐消失，而工业产量则会接近满负荷。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.88s/it, Loss=4.6139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): This trend puts annual GDP growth on course to average only 1.3% through 2025, implying a third consecutive decade of stagnation.\n",
      "FIND EOS\n",
      "翻译结果: 这一种一个新的好处，并将会被视为到一个新的一种，并将会导致对其成本，而不是持续的增长，并将会获得一种。<eos>\n",
      "参考翻译: 这一趋势意味着到2025年，日本年GDP增长率平均将只有1.3 % ， 连续第三个十年陷入停滞。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.91s/it, Loss=4.4380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): More than half of this growth increment can be met by adopting best practices that companies around the world already use, while technology can close much of the remaining gap.\n",
      "FIND EOS\n",
      "翻译结果: 而这一种全球变暖的研究，而不是是因为它的经济增长，而这些国家在世界经济的经济增长的世界贸易自由化的经济的。<eos>\n",
      "参考翻译: 一大半增长增量可以通过采取全球公司已在使用的最佳实践实现，而技术可以填补大部分余下的空缺。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=4.2363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Japan is the only G7 country that has a territorial dispute with Russia, and it is clearly more eager to reach a deal than the Kremlin is.\n",
      "FIND EOS\n",
      "翻译结果: 我们是一个一个更多的，并一个一个新的，并在这一种，并在这一种更多的政治和社会。<eos>\n",
      "参考翻译: 日本是唯一一个与俄罗斯存在领土纠纷的G7国家，并且它显然比克里姆林宫更渴望达成协议。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=4.1013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): By the 1970’s, the neoclassical general-equilibrium school captured Keynesian economics through real-sector models that assumed that “finance is a veil,” thereby becoming blind to financial markets’ destabilizing effects.\n",
      "FIND EOS\n",
      "翻译结果: 美国政府在美国的问题，“使”（比如）”），并”了一个新）的“金融市场”，”（如）”）”）。<eos>\n",
      "参考翻译: 到了1970年代，新古典主义总体均衡学派通过一个假设“金融只是一张包装纸”的实体部门模型说服了凯恩斯主义者，并因此令他们对金融市场的不稳定作用一无所知。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.88s/it, Loss=4.0098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Of course, Abe is far from the first Japanese leader to confront this challenge.\n",
      "FIND EOS\n",
      "翻译结果: 当然，这一问题是如此，这一种是因为它的事情。<eos>\n",
      "参考翻译: 当然，安倍绝非首位面临如此挑战的日本领导人。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=3.8590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): These are some of the poorest households in the world, and, ironically, some of the hungriest as well, despite being food producers.\n",
      "FIND EOS\n",
      "翻译结果: 但在世界上，世界最大程度上的国家，而不是世界最大程度的世界最大程度上最大的世界最大的国家和世界最贫困、世界最贫困的。<eos>\n",
      "参考翻译: 颇具讽刺意味的是，尽管自身也生产粮食，但他们却隶属于世界最贫困、最饥饿家庭的行列。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.90s/it, Loss=3.5794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): NEW YORK – In the afternoon of July 16 two men appeared to be breaking into a fine house in an expensive area of Cambridge, Massachusetts.\n",
      "FIND EOS\n",
      "翻译结果: 发自纽约—————如果我总统纽约了这一种一种一次到访——————这是一个新的新的人——而不是经济学入门。<eos>\n",
      "参考翻译: 纽约 — — 7月16日下午两名男子看似正试图闯入位于马萨诸塞州坎布里奇高尚住宅区的一栋豪宅。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=3.6270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): It is probably true that many white policemen, even if they are trained to avoid racial profiling, as Sgt. Crowley was, need to be convinced that a black man can be at home in one of the finer houses of Cambridge, or any other American city.\n",
      "FIND EOS\n",
      "翻译结果: 但是，如果没有什么，我们可以用在他的情况下，但如果想不能激发都能让我们就能让我们就能让我们所带来的问题。<eos>\n",
      "参考翻译: 也许的确需要说服像克劳利中士这样的许多白人警官，即便他们曾经接受过避免暴力执法的训练，黑人也可以居住在位于坎布里奇、或者其他任何美国城市的高尚住宅区的家中。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.88s/it, Loss=3.3957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): And, given the current rage for all things Bollywood in France, a lavish Indian wedding would be fitting.\n",
      "FIND EOS\n",
      "翻译结果: 但是，如果印度的日本将在印度和印度总统的时候，但现在在印度和中亚将会带来了更具人，而不是对俄了我们的政治和印度。<eos>\n",
      "参考翻译: 而且，鉴于法国目前对于印度的宝莱坞电影着迷，一场浪漫的印度婚礼将会再合适不过了。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.89s/it, Loss=3.2510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Pollution would be reduced by 70% as a result of fewer nitrates in the surface water, and greenhouse gases would be cut.\n",
      "FIND EOS\n",
      "翻译结果: 首先，将会将继续和企业将会将继续扩大削减，并将继续的情况下，并将会受到了麻烦。<eos>\n",
      "参考翻译: 同时，得益于地表水中硝酸盐含量的降低，污染将被削减七成，温室气体也会减少。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.91s/it, Loss=3.3769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): They could make a real contribution to cleaner, more productive, and efficient farming and land use, while addressing social needs.\n",
      "FIND EOS\n",
      "翻译结果: 它们可以可以肯定、更加多产、更多的农业政策和更多的决策，并更加更多的土地的方式来为更多的、更多的农业政策。<eos>\n",
      "参考翻译: 它们将对更为清洁、更加多产并且高效的农业与土地使用做出真正的贡献，并能兼顾社会的需求。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|████████████████████████████████████████| 32/32 [01:04<00:00,  2.02s/it, Loss=3.0372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): These pooled funds would enable farmers in poor countries to obtain the fertilizer, improved seed varieties, and small-scale irrigation equipment that they urgently need.\n",
      "FIND EOS\n",
      "翻译结果: 这些国家的科学应该将在今后国家的化肥、化肥、化肥、化肥和其他贫困的领域化肥和其他贫困的领域。<eos>\n",
      "参考翻译: 这些集中起来的资金能让贫穷国家的农民获得急需的化肥、良种和小型灌溉设备。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|████████████████████████████████████████| 32/32 [01:02<00:00,  1.97s/it, Loss=3.0172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): In the health-care sector, it may be more profitable to devote research to a “me-too” drug than to the development of a treatment that really makes a difference.\n",
      "FIND EOS\n",
      "翻译结果: 在欧盟中，将会让欧洲大陆更多的“高”系统”，而这些国家将是通过更多的一次历史性发展目标。<eos>\n",
      "参考翻译: 在卫生领域，致力于研究“仿造”药可能比开发一种另辟蹊径的疗法更加有利可图。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.92s/it, Loss=2.7704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Then, a few years ago, a group of young writers – such as Chetan Bhagat and Amish Tripathi, both former bankers – changed the rules of the game by writing for the mass market.\n",
      "翻译结果: 与此同时，一种国——比如，拘捕和韩国—比如，通过整洁下是一种合适的贡献——比如下，是设立是人——而不是是由一名了一代人的做事了环境—的后果\n",
      "参考翻译: 但是，几年前，一群青年作家 — — 如Chetan Bhagat和Amish Tripathi，两人此前都是银行家 — — 通过为大众写作改变了游戏规则。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|████████████████████████████████████████| 32/32 [01:00<00:00,  1.90s/it, Loss=2.6813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): That neglect is a human tragedy.\n",
      "FIND EOS\n",
      "翻译结果: 这一种是指的是不能我们。<eos>\n",
      "参考翻译: 这一疏忽是人类的悲剧。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|████████████████████████████████████████| 32/32 [01:01<00:00,  1.93s/it, Loss=2.4840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Making Do With More\n",
      "FIND EOS\n",
      "翻译结果: 心不在焉更多的更多更多得更多得更多的杀戮者。<eos>\n",
      "参考翻译: 多劳应多得\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|████████████████████████████████████████| 32/32 [01:10<00:00,  2.20s/it, Loss=2.6981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): We experts should have to explain ourselves.\n",
      "FIND EOS\n",
      "翻译结果: 我们我们应当我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们自己我们我们我们我们自己我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们我们自己。<eos>\n",
      "参考翻译: 我们这些专家应当向外界解释我们自己。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|████████████████████████████████████████| 32/32 [01:08<00:00,  2.14s/it, Loss=2.3717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): I protested that it would surely be better for the universities to choose their own constitutional heads.\n",
      "FIND EOS\n",
      "翻译结果: 让恐怖分子也没有什么纯粹地带的新被武装。<eos>\n",
      "参考翻译: 我抗议说应当让大学自主选择行政领导。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|████████████████████████████████████████| 32/32 [01:08<00:00,  2.13s/it, Loss=2.1912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Meanwhile, universities in China and Hong Kong are faced with threats to their autonomy and freedom, not from within, but from an authoritarian government.\n",
      "FIND EOS\n",
      "翻译结果: 此外，政府和其他地方的科学，因为它们还必须重新定义和董事会成员，因为它们还不像它们和其他地方的学术衣钵。<eos>\n",
      "参考翻译: 此外，中国和香港大学的自主权和自由正面临威胁，这些威胁并非来自于学校内部，而是来自于独裁政府。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|████████████████████████████████████████| 32/32 [01:49<00:00,  3.43s/it, Loss=2.0879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): To be sure, there is nothing wrong with scientific practices evolving along with technology, or with academics using richer data sets and better software.\n",
      "FIND EOS\n",
      "翻译结果: 更具、技术、抑或和其他更多的人能够或企业或犯或C级或欠更多的人实践、更多的不平等其他更多的企业或C级更多的人震惊更好的。<eos>\n",
      "参考翻译: 可以肯定，科学实践随技术演进、抑或学术界使用更丰富的数据集和更先进的软件都属于正常现象。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|████████████████████████████████████████| 32/32 [02:46<00:00,  5.21s/it, Loss=1.9413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): Good data are also essential to the effectiveness of advocacy groups as they push providers to improve their services.\n",
      "FIND EOS\n",
      "翻译结果: 好数据这些机构也有2万3千供应商流程和其他国家地位机构也得以。<eos>\n",
      "参考翻译: 好数据对于利益推动组织敦促供应商改善服务的效率也至关重要。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|████████████████████████████████████████| 32/32 [01:51<00:00,  3.50s/it, Loss=1.8567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): But a growing body of evidence suggests that these views are misplaced.\n",
      "FIND EOS\n",
      "翻译结果: 这些国家的一个是，但这样的观点并试图正确对这些农民的变化，而这些着一种相互依存的变化。<eos>\n",
      "参考翻译: 但越来越多的证据表明，这样的观点并不正确。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|████████████████████████████████████████| 32/32 [01:24<00:00,  2.63s/it, Loss=1.7623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): The environmental destruction of the delta is part of a larger saga: corrupt companies operating hand in hand with corrupt government officials.\n",
      "翻译结果: 所有公司的一个是如此的：或者行为的政府官员公司（IntergovernmentalPanel）在欣欣向荣的政府官员）中），企业发起人的不平等的政府和年仅43——也就是说，以及对这一种的真实坎坷的后果\n",
      "参考翻译: 三角洲的环境破坏只是一长串事件中的一部分：腐败的公司与腐败的政府官员相互勾结以牟取私利。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|████████████████████████████████████████| 32/32 [01:09<00:00,  2.18s/it, Loss=1.6233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): We generally overlook our actions’ spillover effects on others, or “externalities.”\n",
      "FIND EOS\n",
      "翻译结果: 我们通常在“竞争性”的东西”或政治或“外部性”或”的人——即进入“我们”或或”。<eos>\n",
      "参考翻译: 我们通常忽略了自身行为对他人的溢出效应或“外部性 ” 。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|████████████████████████████████████████| 32/32 [01:21<00:00,  2.54s/it, Loss=1.5213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): A Centerless Euro Cannot Hold\n",
      "FIND EOS\n",
      "翻译结果: 我们应该更国家的社会并非得不能让国家更好的更好的更好的社会无法适应更好的，并不能让。<eos>\n",
      "参考翻译: 缺少集中的欧元无法生存\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|████████████████████████████████████████| 32/32 [01:18<00:00,  2.45s/it, Loss=1.3873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): European policymakers today often complain that, were it not for the US financial crisis, the eurozone would be doing just fine.\n",
      "FIND EOS\n",
      "翻译结果: 但是，欧洲决策者不会消失，就像金融危机，但金融危机，人们必须解决方案，而不是危机，那么我们必须金融危机前金融危机都不会引发金融危机并专注于。<eos>\n",
      "参考翻译: 如今，欧洲决策者经常抱怨说，要不是美国金融危机，欧元区根本不会遭殃。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|████████████████████████████████████████| 32/32 [01:19<00:00,  2.50s/it, Loss=1.4194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "测试句子 (英文): That doesn’t mean that we are advocating a move to socialism; we want better and smarter regulation, not regulation for its own sake.\n",
      "FIND EOS\n",
      "翻译结果: 我们并不意味着我们需要我们需要更多的更多地；我们无法获得和监管而监管。<eos>\n",
      "参考翻译: 这并不意味着我们鼓吹社会主义；监管必须更好和更明智，而不是为监管而监管。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42:  12%|█████▏                                   | 4/32 [00:23<02:41,  5.77s/it, Loss=1.2980]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     85\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\21198\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\21198\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\21198\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prepared dataloader and model! start train\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "src_vocab_size = len(en_vocab)\n",
    "trg_vocab_size = len(zh_vocab)\n",
    "\n",
    "# record loss with tensorboard\n",
    "writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    num_heads,\n",
    "    dropout,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    pad_idx,\n",
    "    dim_feedforward,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, \n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "    \n",
    "    # 获取验证集样本用于翻译测试\n",
    "    test_sample = train_dataset[step]['translation']\n",
    "    test_en_sentence = test_sample['en']\n",
    "    test_zh_reference = test_sample['zh']\n",
    "    print(f\"测试句子 (英文): {test_en_sentence}\")\n",
    "    # 每个epoch进行翻译测试\n",
    "    translated = translate_sentence(model, test_en_sentence, en_vocab, zh_vocab, device)\n",
    "    print(f\"翻译结果: {translated}\")\n",
    "    print(f\"参考翻译: {test_zh_reference}\\n\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), \n",
    "                       total=len(train_dataloader),\n",
    "                       desc=f\"Epoch {epoch+1}\",\n",
    "                       ncols=100)\n",
    "    \n",
    "    for _, (en_batch, zh_batch) in progress_bar:\n",
    "        input = en_batch.to(device)\n",
    "        target = zh_batch.to(device)\n",
    "        # for i, (en_tokens, zh_tokens) in enumerate(zip(input, target)):\n",
    "        #     print(f'[pair{i+1}]')\n",
    "        #     print(f'English: {translate_to_sentence(en_tokens, en_vocab, is_chinese=False)}')\n",
    "        #     print(f'Chinese: {translate_to_sentence(zh_tokens, zh_vocab, is_chinese=True)}')\n",
    "        #     print()  # 添加空行便于阅读\n",
    "\n",
    "        #     # 只打印前几个样本避免输出过多\n",
    "        #     if i >= 3:\n",
    "        #         break\n",
    "            \n",
    "\n",
    "        right_shift = torch.ones(target.shape[0], 1, dtype=target.dtype, device=target.device)\n",
    "        shifted_target = torch.cat([right_shift, target], dim=1)\n",
    "        \n",
    "        output = model(input, shifted_target)\n",
    "\n",
    "        output = output[:,:-1,:].reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "        progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
